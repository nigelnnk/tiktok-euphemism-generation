{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4Q6usoSwUh6"
      },
      "source": [
        "In this notebook, we demonstrate how to extract and lookup for contextually-most-similar words using BERT and nearest neighbor search. \n",
        "\n",
        "This was inspired by the StackOverflow question https://stackoverflow.com/questions/59865719/how-to-find-the-closest-word-to-a-vector-using-bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZveD1Oe68wCi"
      },
      "source": [
        "Note: The environment we use is google colab, first run. Subsequent runs after reset have had issues with dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ul0lMapo0T3"
      },
      "source": [
        "# learn to extract embeddings from bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTeM3tJxhDd6"
      },
      "source": [
        "We use `bert-embedding` package; see https://pypi.org/project/bert-embedding/\n",
        "\n",
        "We use GPU, so please choose the Colab kernel accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G7EZJcvygjTI",
        "outputId": "743ff202-0411-4ed2-a447-ee1fae2a7d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mxnet-cu102 in /usr/local/lib/python3.8/dist-packages (1.9.1)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from mxnet-cu102) (2.23.0)\n",
            "Collecting numpy<2.0.0,>1.16.0\n",
            "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from mxnet-cu102) (0.8.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet-cu102) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet-cu102) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet-cu102) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet-cu102) (2022.9.24)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.23.5 which is incompatible.\n",
            "mxnet 1.4.0 requires numpy<1.15.0,>=1.8.2, but you have numpy 1.23.5 which is incompatible.\n",
            "bert-embedding 1.0.1 requires numpy==1.14.6, but you have numpy 1.23.5 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bert-embedding in /usr/local/lib/python3.8/dist-packages (1.0.1)\n",
            "Requirement already satisfied: gluonnlp==0.6.0 in /usr/local/lib/python3.8/dist-packages (from bert-embedding) (0.6.0)\n",
            "Requirement already satisfied: mxnet==1.4.0 in /usr/local/lib/python3.8/dist-packages (from bert-embedding) (1.4.0)\n",
            "Requirement already satisfied: typing==3.6.6 in /usr/local/lib/python3.8/dist-packages (from bert-embedding) (3.6.6)\n",
            "Collecting numpy==1.14.6\n",
            "  Using cached numpy-1.14.6-cp38-cp38-linux_x86_64.whl\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from mxnet==1.4.0->bert-embedding) (2.23.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from mxnet==1.4.0->bert-embedding) (0.8.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (3.0.4)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires numpy>=1.16.0, but you have numpy 1.14.6 which is incompatible.\n",
            "xarray 0.20.2 requires numpy>=1.18, but you have numpy 1.14.6 which is incompatible.\n",
            "xarray-einstats 0.3.0 requires numpy>=1.19, but you have numpy 1.14.6 which is incompatible.\n",
            "tifffile 2022.10.10 requires numpy>=1.19.2, but you have numpy 1.14.6 which is incompatible.\n",
            "thinc 8.1.5 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "tensorflow 2.9.2 requires numpy>=1.20, but you have numpy 1.14.6 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.14.6 which is incompatible.\n",
            "statsmodels 0.12.2 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "spacy 3.4.3 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "seaborn 0.11.2 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.14.6 which is incompatible.\n",
            "scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.14.6 which is incompatible.\n",
            "resampy 0.4.2 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "pywavelets 1.4.1 requires numpy>=1.17.3, but you have numpy 1.14.6 which is incompatible.\n",
            "pymc 4.1.4 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "pyarrow 9.0.0 requires numpy>=1.16.6, but you have numpy 1.14.6 which is incompatible.\n",
            "prophet 1.1.1 requires numpy>=1.15.4, but you have numpy 1.14.6 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.14.6 which is incompatible.\n",
            "pandas 1.3.5 requires numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you have numpy 1.14.6 which is incompatible.\n",
            "pandas-gbq 0.17.9 requires numpy>=1.16.6, but you have numpy 1.14.6 which is incompatible.\n",
            "opencv-python 4.6.0.66 requires numpy>=1.17.3; python_version >= \"3.8\", but you have numpy 1.14.6 which is incompatible.\n",
            "opencv-python-headless 4.6.0.66 requires numpy>=1.17.3; python_version >= \"3.8\", but you have numpy 1.14.6 which is incompatible.\n",
            "opencv-contrib-python 4.6.0.66 requires numpy>=1.17.3; python_version >= \"3.8\", but you have numpy 1.14.6 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.14.6 which is incompatible.\n",
            "mxnet-cu102 1.9.1 requires numpy<2.0.0,>1.16.0, but you have numpy 1.14.6 which is incompatible.\n",
            "librosa 0.8.1 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.14.6 which is incompatible.\n",
            "jaxlib 0.3.25+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.14.6 which is incompatible.\n",
            "jax 0.3.25 requires numpy>=1.20, but you have numpy 1.14.6 which is incompatible.\n",
            "imgaug 0.4.0 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "httpstan 4.6.1 requires numpy<2.0,>=1.16, but you have numpy 1.14.6 which is incompatible.\n",
            "h5py 3.1.0 requires numpy>=1.17.5; python_version == \"3.8\", but you have numpy 1.14.6 which is incompatible.\n",
            "gym 0.25.2 requires numpy>=1.18.0, but you have numpy 1.14.6 which is incompatible.\n",
            "db-dtypes 1.0.4 requires numpy<2.0dev,>=1.16.6, but you have numpy 1.14.6 which is incompatible.\n",
            "cvxpy 1.2.2 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "cmdstanpy 1.0.8 requires numpy>=1.21, but you have numpy 1.14.6 which is incompatible.\n",
            "blis 0.7.9 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "aesara 2.7.9 requires numpy>=1.17.0, but you have numpy 1.14.6 which is incompatible.\n",
            "aeppl 0.0.33 requires numpy>=1.18.1, but you have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.14.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install mxnet-cu102\n",
        "!pip install bert-embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBbQwlK9gtsD"
      },
      "outputs": [],
      "source": [
        "import mxnet as mx\n",
        "from bert_embedding import BertEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aa3Lb7LfhBCU"
      },
      "outputs": [],
      "source": [
        "# Can't get this to work... may be deprecated\n",
        "# ctx = mx.gpu(0)\n",
        "bert = BertEmbedding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxAdRCkFmq5R"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm, trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrH9e6AIhXy5"
      },
      "outputs": [],
      "source": [
        "bert_abstract = \"\"\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.\n",
        " Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.\n",
        " As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \n",
        "BERT is conceptually simple and empirically powerful. \n",
        "It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human performance by 2.0%.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM0FmfpphZP6",
        "outputId": "8e389730-6bad-42ba-d1e1-496140428dce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['we', 'introduce', 'a', 'new', 'language', 'representation', 'model', 'called', 'bert', ',', 'which', 'stands', 'for', 'bidirectional', 'encoder', 'representations', 'from', 'transformers']\n",
            "18 18\n",
            "[ 0.47964773  0.1824888  -0.28597528 -0.46567446  0.01248981 -0.07430505\n",
            " -0.18017295  0.37813222  0.9135139  -0.25295883]\n"
          ]
        }
      ],
      "source": [
        "sentences = bert_abstract.split('\\n')\n",
        "result = bert(sentences)\n",
        "toks, embs = result[0]\n",
        "print(toks)\n",
        "print(len(toks), len(embs))\n",
        "print(embs[0][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR-AMiRKoxOP"
      },
      "source": [
        "# process a corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_hJoYtvmZw2"
      },
      "source": [
        "We download a 10k web-public .com corpus from https://wortschatz.uni-leipzig.de/en/download/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d_fuzIOngqC",
        "outputId": "682f4234-95df-476e-c979-ae79588db875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-08 10:44:44--  https://files.pushshift.io/gab/GABPOSTS_2018-10.xz\n",
            "Resolving files.pushshift.io (files.pushshift.io)... 172.67.170.36, 104.21.28.11, 2606:4700:3031::6815:1c0b, ...\n",
            "Connecting to files.pushshift.io (files.pushshift.io)|172.67.170.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 337144496 (322M) [application/octet-stream]\n",
            "Saving to: ‘GABPOSTS_2018-10.xz.1’\n",
            "\n",
            "GABPOSTS_2018-10.xz 100%[===================>] 321.53M  15.9MB/s    in 19s     \n",
            "\n",
            "2022-12-08 10:45:03 (16.8 MB/s) - ‘GABPOSTS_2018-10.xz.1’ saved [337144496/337144496]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://files.pushshift.io/gab/GABPOSTS_2018-10.xz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zashEvfGnnnK"
      },
      "outputs": [],
      "source": [
        "import lzma\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "gab_posts = pd.DataFrame()\n",
        "temp = []\n",
        "counter = 0\n",
        "with lzma.open('GABPOSTS_2018-10.xz', mode='r') as file:\n",
        "    for line in file:\n",
        "      # Can add raw/other fields, just worried about memory requirements\n",
        "      # I think I did this in a cleaner way before, but w/e it works :)\n",
        "      if counter > 100000:\n",
        "        break\n",
        "      counter = counter + 1\n",
        "      temp.append({\"body\": re.sub(r\"(?:\\@|\\\\|https?\\://)\\S+\", \"\",json.loads(line)['body']), \"date\":json.loads(line)['created_at']})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCWGTugrocOZ"
      },
      "outputs": [],
      "source": [
        "gab_posts = gab_posts.append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFMT77_f8hUk",
        "outputId": "64fa55a2-999c-42a6-8788-a9a0363f1262"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5001"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "len(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kofx-oobsKB-"
      },
      "outputs": [],
      "source": [
        "banned_words = ['pussie', 'phonesex', 'footjob', 'horniest', 'clitoris', 'headfuck', 'areola', 'pussies', 'goddamnes', 'suicide', 'voyeurweb', 'suicide girls', 'niggarded', 'deepthroat', 'fuckbuddy', 'nigra', 'freefuck', 'boob', 'hentai', 'rentafuck', 'wanking', 'jerk off', 'molester', 'horney', 'titfuckin', 'milf', 'wrapping men', 'whorefucker', 'masturbating', 'dick', 'honkers', 'chocolate rosebuds', 'neonazi', 'vibrater', 'uptheass', 'shitdick', 'pussycat', 'naked', 'group sex', 'suckmydick', 'pussyeater', 'masturbate', 'stupidfuck', 'nig', 'rape', 'meth', 'virgin', 'livesex', 'terrorist', 'upskirt', 'shortfuck', 'genital', 'jiggaboo', 'marijuana', 'cumshots', 'koon', 'holestuffer', 'tit', 'assbagger', 'ball sack', 'sexpot', 'suckmyass', 'lovejuice', 'phukking', 'wigger', 'black cock', 'whiskeydick', 'blonde on blonde action', 'retarded', 'kunt', 'motherfuckin', 'orgy', 'ejaculation', 'fuckme', 'phone sex', 'fuckher', 'niggerhole', 'intercourse', 'pussylips', 'niggardly', 'tongethruster', 'nig nog', 'kumbullbe', 'nigger', 'wanker', 'peepshpw', 'cocks', 'omorashi', 'female squirting', 'blow job', 'bung hole', 'homicide', 'penetration', 'puddboy', 'gang bang', 'lickme', 'spermhearder', 'titties', 'rigger', 'shitblimp', 'twat', 'fag', 'gangbanger', 'orgasim', 'porno', 'assfuck', 'pussy', 'sodomy', 'cumshot', 'cock', 'jihad', 'niggaz', 'picaninny', 'bondage', 'dry hump', 'poorwhitetrash', 'whitenigger', 'nip', 'masturbation', 'peni5', 'sexed', 'escort', 'g-spot', 'muffindiver', 'fingerbang', 'shite', 'gypo', 'scrotum', 'creampie', 'goddamnmuthafucker', 'foreskin', 'titty', 'dildo', 'sexkitten', 'anus', 'niggling', 'niggerhead', 'footlicker', 'pussylover', 'limpdick', 'fucktard', 'male squirting', 'gangbang', 'nigg', 'suckdick', 'vagina', 'reestie', 'bangbros', 'givehead', 'spank', 'trailertrash', 'giant cock', 'fucktards', 'sexo', 'pussypounder', 'gaymuthafuckinwhore', 'negroid', 'lsd', 'ball gag', 'jigga', \"nigger's\", 'orgasm', 'nlgger', 'asskiss', 'coprolagnia', 'boobs', 'pussylicker', 'whitetrash', 'mothafuckings', 'fingering', 'scum', 'paedophile', 'sperm', 'testicle', 'poopchute', 'wank', 'jerkoff', 'octopussy', 'pedophile', 'reverse cowgirl', 'negroes', 'suckmytit', 'big tits', 'sonofbitch', 'swastika', 'jizz', 'sexslave', 'bunghole', 'retard', 'hore', 'nipplering', 'kink', 'nipples', 'vaginal', 'tittie', 'hitler', 'jiggabo', 'pedobear', 'handjob', 'pubic', 'kkk', 'niggled', 'pthc', \"negro's\", 'doggystyle', 'samckdaddy', 'gangbanged', 'clit', 'hand job', 'beaners', 'ecchi', 'doggy style', 'nutten', 'bdsm', 'cunnilingus', 'killing', 'genitals', 'poop chute', 'fuckfest', 'spermherder', 'brunette action', 'motherfuck', 'cumming', 'erotic', 'splooge moose', 'foursome', 'niglet', 'nigre', 'incest', 'cunt', 'molest', 'threesome', 'kissass', 'narcotic', 'sexhouse', 'nudity', 'fudgepacker', 'snownigger', 'white power', 'jiggerboo', 'honky', 'rosy palm and her 5 sisters', 'nittit', 'horny', 'hotpussy', 'ball sucking', 'nignog', 'palesimian', 'jizjuice', 'zoophilia', 'nigga', 'asslicker', 'niggle', 'nlggor', 'pornography', 'sexing', 'slutt', 'titlicker', 'kunnilingus', 'fuckwhore', 'wet dream', 'spunk', 'pisser', 'puss', 'boner', 'skeet', 'sextoys', 'vibrator', 'manpaste', 'faggot', 'humping', 'nipple', 'double penetration', 'coons', 'assklown', 'pubes', 'fuckface', 'anal', 'nimphomania', 'blowjob', 'rimjob', 'fisting', 'niggardliness', 'sultry women', 'jizzim', 'kinkster', 'skankfuck', 'penis', 'how to kill', 'semen', 'mothafucker', 'analsex', 'niggur', 'panty', 'deep throat', 'foot fetish', 'freakyfucker', 'date rape', 'assblaster', 'bukkake', 'lesbo', 'spaghettinigger', 'beaner', 'clover clamps', 'twobitwhore', 'nigr', 'fuckfriend', 'sextoy', 'prostitute', 'pussyfucker', 'kanake', 'porchmonkey', 'testicles', 'erotism', 'pusy', 'assjockey', 'pimpjuic', 'booty call', 'kaffir', 'fuckable', 'goldenshower', 'homobangers', 'pegging', 'rapist', 'venus mound', 'raping', 'fudge packer', 'sexcam', 'timbernigger', 'viagra', 'make me come', 'beastiality', 'leather restraint', 'coon', 'futanari', 'fuckina', 'iblowu', 'masterbate', 'luckycammeltoe', \"niggardliness's\", 'fuckmehard', 'tits', 'suckme', 'intheass', 'niggarding', 'tonguetramp', 'niggor', 'schlong', 'niggah', 'raped', 'nazi', 'two girls one cup', 'huge fat', 'upthebutt', 'daterape', 'mastabater', 'cum', 'asslick', 'raghead', 'bestiality', 'golden shower', 'niggers', 'penises', 'mufflikcer', 'camel toe', 'shaved pussy', 'niggles', 'jijjiboo']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLVWcfdwsOxt",
        "outputId": "84c483f0-cee6-40f4-a9f2-3695a377c580"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                      #trade #winning   \n",
              "1       o deputado arthur lira (pp/al):  ⚠️ responde a...\n",
              "2                     cocaine mitch comes out swinging.  \n",
              "3       #demoncrats have no redeeming value. a black c...\n",
              "4       putting up the rent in one town/city/country w...\n",
              "                              ...                        \n",
              "4996                            is this true - holy shit!\n",
              "4997    #texasfirst #jesuskills #ziohomophobia #martin...\n",
              "4998    just in: feinstein calls on white house, fbi t...\n",
              "4999                                  keep thinking that \n",
              "5000    huh!! ((((gasp))) gt!!! those are my puppiesss...\n",
              "Name: body, Length: 4289, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "\n",
        "gab_posts['body'].replace(r'\\n',' ', regex=True, inplace=True)\n",
        "gab_posts['body'].replace(r'\\r',' ', regex=True, inplace=True)\n",
        "gab_posts['body'].replace('', np.nan, inplace=True)\n",
        "gab_posts.dropna(subset=['body'], inplace=True)\n",
        "gab_posts['body'].str.lower()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QghFckJrumO"
      },
      "source": [
        "remove row index from each sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7FTLEtgrlGE"
      },
      "outputs": [],
      "source": [
        "all_sentences = gab_posts['body'].to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOLT6c7yotcW"
      },
      "source": [
        "# create a search index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZvELpMSsyer"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7MAHVbbsz3t"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KDTree\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ContextNeighborStorage:\n",
        "    def __init__(self, sentences, model):\n",
        "        self.sentences = sentences\n",
        "        self.model = model\n",
        "\n",
        "    def process_sentences(self):\n",
        "        result = self.model(self.sentences)\n",
        "\n",
        "        self.sentence_ids = []\n",
        "        self.token_ids = []\n",
        "        self.all_tokens = []\n",
        "        all_embeddings = []\n",
        "        for i, (toks, embs) in enumerate(tqdm(result)):\n",
        "            for j, (tok, emb) in enumerate(zip(toks, embs)):\n",
        "                self.sentence_ids.append(i)\n",
        "                self.token_ids.append(j)\n",
        "                self.all_tokens.append(tok)\n",
        "                all_embeddings.append(emb)\n",
        "        all_embeddings = np.stack(all_embeddings)\n",
        "        # we normalize embeddings, so that euclidian distance is equivalent to cosine distance\n",
        "        self.normed_embeddings = (all_embeddings.T / (all_embeddings**2).sum(axis=1) ** 0.5).T\n",
        "\n",
        "    def build_search_index(self):\n",
        "        # this takes some time\n",
        "        self.indexer = KDTree(self.normed_embeddings)\n",
        "\n",
        "    def query(self, query_sent, query_word, k=10, filter_same_word=False):\n",
        "        toks, embs = self.model([query_sent])[0]\n",
        "\n",
        "        found = False\n",
        "        for tok, emb in zip(toks, embs):\n",
        "            if tok == query_word:\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            raise ValueError('The query word {} is not a single token in sentence {}'.format(query_word, toks))\n",
        "        emb = emb / sum(emb**2)**0.5\n",
        "\n",
        "        if filter_same_word:\n",
        "            initial_k = max(k, 100)\n",
        "        else:\n",
        "            initial_k = k\n",
        "        di, idx = self.indexer.query(emb.reshape(1, -1), k=initial_k)\n",
        "        distances = []\n",
        "        neighbors = []\n",
        "        contexts = []\n",
        "        for i, index in enumerate(idx.ravel()):\n",
        "            token = self.all_tokens[index]\n",
        "            if filter_same_word and (query_word in token or token in query_word):\n",
        "                continue\n",
        "            distances.append(di.ravel()[i])\n",
        "            neighbors.append(token)\n",
        "            contexts.append(self.sentences[self.sentence_ids[index]])\n",
        "            if len(distances) == k:\n",
        "                break\n",
        "        return distances, neighbors, contexts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIiBCBOauY6J"
      },
      "source": [
        "Now let's use this indexer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "27ee02f7c83441dda9739228a11b2a15",
            "7d954ce4917e425588a1f5ec4e44eb59",
            "0609395aa59a455eabd6169db701e237",
            "8bd48da0c2674e6c91f842f0d479b56c",
            "71cfe2496ad04fc2ab56556993fbb497",
            "a2020d8eb5f2421ab1380e0413f54cf1",
            "79382c54e10244229829e9b96679f993",
            "2e745f55333245f68d6846ef73f265f0",
            "fe21129cee3b44c8a8318a2d3485da55",
            "e8f3afcd4d2e49f6864fda70350a1efc",
            "04fa568012c545dfb8d8cc92f2bbbcd3"
          ]
        },
        "id": "rB5NoR7AqJ6a",
        "outputId": "65e1482a-7814-4a62-f9f4-ecb0d939b0b8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4289 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27ee02f7c83441dda9739228a11b2a15"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "storage = ContextNeighborStorage(sentences=all_sentences, model=bert)\n",
        "storage.process_sentences()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_sEJcvtzPs5"
      },
      "source": [
        "Creating the index would require some time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWcGzTNxuJm9"
      },
      "outputs": [],
      "source": [
        "storage.build_search_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBWJH5W0Sf38"
      },
      "outputs": [],
      "source": [
        "banned_sentences = dict((word,[]) for word in banned_words)\n",
        "\n",
        "# Find all banned sentences in posts\n",
        "for idx, entry in gab_posts.iterrows():\n",
        "  body = entry['body']\n",
        "  for word in banned_words:\n",
        "    # Realize this needs to be exact match, it gets results without though\n",
        "    if word in body:\n",
        "      banned_sentences[word].append(body)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naPQ_LhmmnSK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14a7135d-ac6f-42c9-9379-81de21443b00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Those are #Muzhood Agent Obama's Faggots he forced in as Generals to Awol pussies,  Where 6 Marines were murdered looking for the fag. \n",
            "The query word pussie is not a single token in sentence ['those', 'are', '#', 'muzhood', 'agent', 'obama', \"'\", 's', 'faggots', 'he', 'forced', 'in', 'as', 'generals', 'to', 'awol', 'pussi']\n",
            "These women were all liars, in my view just like the Duke Lacross, Twanna brawley lie, the liars against Trump that were paid $750,000,the Judge Roy Moore accusers that are about to go to trial. All of them. It is weaponized old pussies. Time to fight!!\n",
            "The query word pussie is not a single token in sentence ['these', 'women', 'were', 'all', 'liars', ',', 'in', 'my', 'view', 'just', 'like', 'the', 'duke', 'lacross', ',', 'twanna', 'brawley', 'lie']\n",
            "Those are #Muzhood Agent Obama's Faggots he forced in as Generals to Awol pussies,  Where 6 Marines were murdered looking for the fag. \n",
            "The query word pussies is not a single token in sentence ['those', 'are', '#', 'muzhood', 'agent', 'obama', \"'\", 's', 'faggots', 'he', 'forced', 'in', 'as', 'generals', 'to', 'awol', 'pussi']\n",
            "These women were all liars, in my view just like the Duke Lacross, Twanna brawley lie, the liars against Trump that were paid $750,000,the Judge Roy Moore accusers that are about to go to trial. All of them. It is weaponized old pussies. Time to fight!!\n",
            "The query word pussies is not a single token in sentence ['these', 'women', 'were', 'all', 'liars', ',', 'in', 'my', 'view', 'just', 'like', 'the', 'duke', 'lacross', ',', 'twanna', 'brawley', 'lie']\n",
            "I think Ms. Ford needs INTENSE security NOT hired by Soros/Deep State people.  I think it's possible she might be Arkancided and either leave  a fake suicide note blaming Kav or making it look like \"right-wing crazies\" offed her. That's the only way she can be of any further use to the Deep State. Any further exposure of her or her past can only hurt Deep State.\n",
            "The query word suicide is not a single token in sentence ['i', 'think', 'ms', '.', 'ford', 'needs', 'intense', 'security', 'not', 'hired', 'by', 'soros', '/', 'deep', 'state', 'people', '.', 'i', 'think', 'it', \"'\", 's']\n",
            "He should be shamed until he commits suicide. People should stand outside of his house with a giant cardboard printout of this tweet. \n",
            "LOL, this guy posting pictures while his entire team of heroes was on their knees begging for mercy and then committing suicide.  :D\n",
            "The query word suicide is not a single token in sentence ['lol', ',', 'this', 'guy', 'posting', 'pictures', 'while', 'his', 'entire', 'team', 'of', 'heroes', 'was', 'on', 'their', 'knees', 'begging', 'for', 'mercy', 'and', 'then', 'committing']\n",
            "Q posted the national suicide prevention hotline phone number:  \":1-800-273-8255  Operators standing by.  Q \"\n",
            "#Bootylicious #sparkplug Ms Dakota Skye (4of4)   #NSFW #babe #beauty #sexy #petite #cute #WhiteGirls #InkedAngel #TinyTits #boobies #butts #booty #SundayFunday #TotalBabe #Hotties #pussy #labia #MeatyFlaps #girls #cheeky #bum #bunda #bleached #ass #SpreadEm #DakotaSkye #DatAss #DreamDate #fanny #heinie #buttocks #WhiteGirlsRock #BottomsUp\n",
            "The query word boob is not a single token in sentence ['#', 'bootylicious', '#', 'sparkplug', 'ms', 'dakota', 'skye', '(', '4of4', ')', '#', 'nsfw', '#', 'babe']\n",
            "#Bootylicious #sparkplug Ms Dakota Skye (3of4)   #NSFW #babe #beauty #sexy #petite #cute #WhiteGirls #InkedAngel #TinyTits #boobies #butts #booty #SundayFunday #TotalBabe #Hotties #pussy #labia #MeatyFlaps #girls #cheeky #bum #bunda #bleached #ass #SpreadEm #DakotaSkye #DatAss #DreamDate #fanny #heinie #buttocks #WhiteGirlsRock #BottomsUp\n",
            "The query word boob is not a single token in sentence ['#', 'bootylicious', '#', 'sparkplug', 'ms', 'dakota', 'skye', '(', '3of4', ')', '#', 'nsfw', '#', 'babe']\n",
            "#Bootylicious #sparkplug Ms Dakota Skye (2of4)   #NSFW #babe #beauty #sexy #petite #cute #WhiteGirls #InkedAngel #TinyTits #boobies #butts #booty #SundayFunday #TotalBabe #Hotties #pussy #labia #MeatyFlaps #girls #cheeky #bum #bunda #bleached #ass #SpreadEm #DakotaSkye #DatAss #DreamDate #fanny #heinie #buttocks #WhiteGirlsRock #BottomsUp\n",
            "The query word boob is not a single token in sentence ['#', 'bootylicious', '#', 'sparkplug', 'ms', 'dakota', 'skye', '(', '2of4', ')', '#', 'nsfw', '#', 'babe']\n",
            "#Bootylicious #sparkplug Ms Dakota Skye (1of4)   #NSFW #babe #beauty #sexy #petite #cute #WhiteGirls #InkedAngel #TinyTits #boobies #butts #booty #SundayFunday #TotalBabe #Hotties #pussy #labia #MeatyFlaps #girls #cheeky #bum #bunda #bleached #ass #SpreadEm #DakotaSkye #DatAss #DreamDate #fanny #heinie #buttocks #WhiteGirlsRock #BottomsUp\n",
            "The query word boob is not a single token in sentence ['#', 'bootylicious', '#', 'sparkplug', 'ms', 'dakota', 'skye', '(', '1of4', ')', '#', 'nsfw', '#', 'babe']\n",
            "Served with a nice plate of Favaa beans...hectors molester\n",
            "We should be happy they do fuck their cousins..it's up to you Brit men to keep your women satisfied lest they marry muzzie and you will all eventually be killed or enslaved, depending on how much muzzie dick and ass you suck. Mad yet? \n",
            "The query word dick is not a single token in sentence ['we', 'should', 'be', 'happy', 'they', 'do', 'fuck', 'their', 'cousins', '.', '.', 'it', \"'\", 's', 'up', 'to', 'you', 'brit', 'men', 'to', 'keep', 'your', 'women']\n",
            "At least I can spell the word \"YOU\".  When you get done fingering your own asshole on line?  You can suck my dick.\n",
            "The query word dick is not a single token in sentence ['at', 'least', 'i', 'can', 'spell', 'the', 'word', '\"', 'you', '\"', '.', 'when', 'you', 'get', 'done', 'fingering', 'your', 'own', 'asshole', 'on', 'line', '?']\n",
            "Guns and hot chicks.................my dick is hard enough to cut diamonds......LOL\n",
            "Any men who are pro-LGBT will stick their dick in crazy, and if they will stick their dick in crazy they will stick it in anything. \n",
            "Aww you sound like you got a dick deep in your ass , if they ain't white they can fuck off unless I'm jwalking and telling them about schlomo owning slaves\n",
            "Fuck Islam and Sharia Law. Both can suck a dick.\n",
            "It's better if you search dick van dyke death hoax, or read gab posts about it.   \n",
            "What's in name?  Lol!! His ancestors were dick sucking losers too I just bet. At least his mom and sisters, unless they are dykes, and adopted him.. \n",
            "That's where I keep photos of my naked ex-wife! She's a looker so they're everywhere by now!\n",
            "#texasFirst ReTweets of FAZ_Politik  #MartinLuther #HureISLAM #Koeterrasse #RoboterSindAuchNurMenschen mustC:  #NAFRI Steinigung in Dortmund mustC:  #BockaufDeutschland [phpGAB] \n",
            "The query word nig is not a single token in sentence ['#', 'texasfirst', 'retweets', 'of', 'faz', '_', 'politik', '#', 'martinluther', '#', 'hureis']\n",
            "#nigwiggery\n",
            "The query word nig is not a single token in sentence ['#', 'nigwiggery']\n",
            "Sunday night watching for all the #patriots        Really fantastic film. Get it while it's still on YT!   #wwg1wga #POTUS #maga #MAGA2020 #thegreatawakening #home_of_the_brave  #American\n",
            "The query word nig is not a single token in sentence ['sunday', 'night', 'watching', 'for', 'all', 'the', '#', 'patriots', 'really', 'fantastic', 'film', '.', 'get', 'it', 'while', 'it', \"'\", 's', 'still', 'on', 'yt', '!']\n",
            "Sunday night watching for all the #patriots        Really fantastic film. Get it while it's still on YT!   #wwg1wga #POTUS #maga #MAGA2020 #thegreatawakening #home_of_the_brave  #American\n",
            "The query word nig is not a single token in sentence ['sunday', 'night', 'watching', 'for', 'all', 'the', '#', 'patriots', 'really', 'fantastic', 'film', '.', 'get', 'it', 'while', 'it', \"'\", 's', 'still', 'on', 'yt', '!']\n",
            "They should set him on fire. Ungrateful nigger.\n",
            "The query word nig is not a single token in sentence ['they', 'should', 'set', 'him', 'on', 'fire', '.', 'ungrateful', 'nigger', '.']\n",
            "Based on my research into Anthropometric Study of Facial Morphology   I declare her - Sociopath with extreme narcissistic overlay tendencies - In other words - She'd be the one to send you to the gas chamber, have a drink and dinner afterwords, and sleep soundly at night\n",
            "The query word nig is not a single token in sentence ['based', 'on', 'my', 'research', 'into', 'anthropometric', 'study', 'of', 'facial', 'morphology', 'i', 'declare', 'her', '-', 'sociopath', 'with', 'extreme', 'narc']\n",
            "woman on a plane means nigger word will be said   THIS IS THAT INSTITUTIONAL racism they were talking about   the trillions spent and lives lost were worth it   #DMTBKA       \n",
            "The query word nig is not a single token in sentence ['woman', 'on', 'a', 'plane', 'means', 'nigger', 'word', 'will', 'be', 'said', 'this', 'is', 'that', 'institutional', 'racism', 'they', 'were', 'talking', 'about', 'the', 'trillions']\n",
            "very moving gt! enjoy the rest of your sunday night\n",
            "The query word nig is not a single token in sentence ['very', 'moving', 'gt', '!', 'enjoy', 'the', 'rest', 'of', 'your', 'sunday', 'night']\n",
            "That's a plan. Goodnight. Good for you.   \n",
            "The query word nig is not a single token in sentence ['that', \"'\", 's', 'a', 'plan', '.', 'goodnight', '.', 'good', 'for', 'you', '.']\n",
            "From what I heard wes bellamy is a nigger\n",
            "The query word nig is not a single token in sentence ['from', 'what', 'i', 'heard', 'wes', 'bellamy', 'is', 'a', 'nigger']\n",
            "Vielleicht eher angebracht auf Grund der taeglichen Migrantenmorde: #wirwerdenjedenTagweniger\n",
            "The query word nig is not a single token in sentence ['vielleicht', 'eher', 'angebracht', 'auf', 'grund', 'der', 'taeglichen', 'migrantenmorde', ':', '#']\n",
            "Dogs can sense niggers hostile intentions.\n",
            "The query word nig is not a single token in sentence ['dogs', 'can', 'sense', 'niggers', 'hostile', 'intentions', '.']\n",
            "WarMachineGoons and LeftistLoons, SinisterSynthesis DeceptiveCartoons, Waging WarOnCivilians morning-night-and-noon, while Bootlickers and Sheeple fiercely swoon like proud-global-robotic-buffoons\n",
            "The query word nig is not a single token in sentence ['warmachinegoons', 'and', 'leftistloons', ',', 'sinistersynthesis', 'deceptivecartoons', ',', 'waging', 'war']\n",
            "If you saw an earlier post he put up a picture ... Knights of the Templar.  Their sign was the Red Cross.\n",
            "The query word nig is not a single token in sentence ['if', 'you', 'saw', 'an', 'earlier', 'post', 'he', 'put', 'up', 'a', 'picture', '.', '.', '.', 'knights', 'of', 'the', 'templar', '.', 'their', 'sign', 'was', 'the']\n",
            "I like AK a lot except for the long days and nights. Well, and Murkowski. I have an Alaskan cruise coming up in 2019 or 2020. Probably 2020.\n",
            "The query word nig is not a single token in sentence ['i', 'like', 'ak', 'a', 'lot', 'except', 'for', 'the', 'long', 'days', 'and', 'nights', '.', 'well', ',', 'and', 'murkowski', '.', 'i', 'have', 'an']\n",
            "Look up \"Neo-Confederate Meeting\" on youtube. It came out last night.\n",
            "The query word nig is not a single token in sentence ['look', 'up', '\"', 'neo', '-', 'confederate', 'meeting', '\"', 'on', 'youtube', '.', 'it', 'came', 'out', 'last', 'night', '.']\n",
            "We did the same thing last night. I had leftovers for lunch today.\n",
            "The query word nig is not a single token in sentence ['we', 'did', 'the', 'same', 'thing', 'last', 'night', '.', 'i', 'had', 'leftovers', 'for', 'lunch', 'today', '.']\n",
            "Never tip a nigger,  they never tip and always want the homey hookup.\n",
            "The query word nig is not a single token in sentence ['never', 'tip', 'a', 'nigger', ',', 'they', 'never', 'tip', 'and', 'always', 'want', 'the', 'homey', 'hookup', '.']\n",
            "FOX NEWS has lost all credibility as a result of this issue. No self-respecting journalist would find this manifestly lying sow credible. And ALL of them did! She had 2 lawyers preparing her and guiding her all the way. Shocking. Deluded, cowardly or under instructions?    Nolte: Fox News Cowards Spread Fake News that Christine Blasey Ford Is ‘Credible’   Your ears are not lying to you. A mere 15 minutes into an all-day event, Chris Wallace, a man who poses as a journalist, the man who hosts one of the oh-so venerated Sunday shows, actually said: \"[Ford’s testimony] is extremely emotional, extremely raw, and extremely credible. No one could listen to her deliver those words and talk about the assault and the impact it had had on her life and not have your heart go out to her. … This is a disaster for the Republicans\".   Let me count the ways…    --She has aligned herself with the far-left.  --She straight-up lied about being afraid to fly.  --She said she wanted anonymity but continually reached out to the far-left Washington Post.  --Her polygraph is a farce.  --Her story has been carefully weaved into a Kafka-esque nightmare no man (even with detailed calendars) can ever escape from.  --Every single one of her witnesses refutes her story — has no memory of the gathering in question or says it doesn’t happen, and this includes a lifelong friend.  --Her team was so desperate to have The Woman Who Wants Anonymity to testify publicly, they turned down the opportunity to have her questioned in private at her home in California — and then lied about it.  --Ford’s therapist’s notes from 2012 also refute her tale, even as the media and Democrats try to gaslight us into believing the opposite. Ford originally claimed four boys tried to rape her when she was in her late teens in the mid-eighties. Now she says it was one rapist and one bystander when she was 15 in the early eighties.  --Ford refused to give her therapist’s notes to the Senate Judiciary Committee.  --In the statement she wrote out in her farce of a polygraph test, Ford crossed out “early 80’s” so it would only read “80’s.”  --Ford told the Committee the “primary impact” of the event occurred during the “four years after” it happened. She goes on to say, “I struggled academically. I struggled very much in Chapel Hill and in College. When I was 17 I went off to college, I had a very hard time.” Note how she skips over two whole years, her junior and senior years in high school; the two school years directly after the attack (unless it did indeed happen in her late teens).  --To later confirm the event did in fact happen in 1982, Ford told the Committee she was able to pin it down to 1982 because she remembered she did not yet have her drivers’ license. But… she also says she doesn’t remember how she got to or from the house party, so how does she know she didn’t drive herself?  --Ford also used Mark Judge’s Safeway job to confirm the 1982 timeline. She testified she saw him working there 6-8 weeks after the attack. She could not yet drive, so her mother drove her there, but for some bizarre reason Ford and her mother entered the Safeway using different doors. (And now mom can’t confirm this happened!)  --Five times during her testimony she mentioned Safeway to verify the date. How could she know such a thing unless it really happened? Well, in his memoir (which began circulating online among Kavanaugh critics in the week before Ford’s testimony) Judge helpfully reveals he was working at the “local supermarket” during the “summer before senior year.”   Dr. Ford’s allegations are not only non-credible — they are ludicrous, a joke…\n",
            "The query word nig is not a single token in sentence ['fox', 'news', 'has', 'lost', 'all', 'credibility', 'as', 'a', 'result', 'of', 'this', 'issue', '.', 'no', 'self', '-', 'respecting', 'journalist', 'would', 'find', 'this', 'manifestly']\n",
            "Sunday night watching for all the #patriots       Really fantastic film. Get it while it's still on YT!   #wwg1wga #POTUS #maga #MAGA2020#thegreatawakening #home_of_the_brave  #American\n",
            "The query word nig is not a single token in sentence ['sunday', 'night', 'watching', 'for', 'all', 'the', '#', 'patriots', 'really', 'fantastic', 'film', '.', 'get', 'it', 'while', 'it', \"'\", 's', 'still', 'on', 'yt', '!']\n",
            "Amen Sister! I had a long talk with God last night. Asked Him to watch over our wonderful President & Judge Kavanaugh & his beautiful family. His wife is a jewel! Her heart breaks for her husband! God grant that they are given grace to endure this fiasco!   \n",
            "The query word nig is not a single token in sentence ['amen', 'sister', '!', 'i', 'had', 'a', 'long', 'talk', 'with', 'god', 'last', 'night', '.', 'asked', 'him', 'to', 'watch', 'over', 'our', 'wonderful', 'president', '&']\n",
            "Sunday night watching for all the #patriots       Really fantastic film. Get it while it's still on YT!   #wwg1wga #POTUS #maga #MAGA2020#thegreatawakening #home_of_the_brave  #American\n",
            "The query word nig is not a single token in sentence ['sunday', 'night', 'watching', 'for', 'all', 'the', '#', 'patriots', 'really', 'fantastic', 'film', '.', 'get', 'it', 'while', 'it', \"'\", 's', 'still', 'on', 'yt', '!']\n",
            "   Have to agree.  What assurances do we have they that will conduct a fair investigation in light of their recent devious shenanigans?\n",
            "The query word nig is not a single token in sentence ['have', 'to', 'agree', '.', 'what', 'assurances', 'do', 'we', 'have', 'they', 'that', 'will', 'conduct', 'a', 'fair', 'investigation', 'in', 'light', 'of', 'their', 'recent', 'devi']\n",
            "But I guess all the old ass White men on the Dems side doesn't bother her at all.  This war on Whites is getting serioulsy out of hand.  It has already cost countless White lives because niggers, spics, even Asians, buy into the bullshit that every single problem in their lives is the fault of the White man, who has everything handed to him on the backs of everyone else.  And then they scream racism.  Holy shit is this getting good.  I can't wait to see us all hanged as modern day witches.  Fun times!!\n",
            "The query word nig is not a single token in sentence ['but', 'i', 'guess', 'all', 'the', 'old', 'ass', 'white', 'men', 'on', 'the', 'dems', 'side', 'doesn', \"'\", 't', 'bother', 'her', 'at', 'all', '.', 'this']\n",
            "18 Weeknight Pastas You Can Magic Out of Anything in Your Pantry \n",
            "The query word nig is not a single token in sentence ['18', 'weeknight', 'pastas', 'you', 'can', 'magic', 'out', 'of', 'anything', 'in', 'your', 'pantry']\n",
            "Is her nigger neck broke?\n",
            "The query word nig is not a single token in sentence ['is', 'her', 'nigger', 'neck', 'broke', '?']\n",
            "I spent a few nights down at that pub overlooking the lock in the beer garden. Can't believe it was the only pub in the village! 😮\n",
            "The query word nig is not a single token in sentence ['i', 'spent', 'a', 'few', 'nights', 'down', 'at', 'that', 'pub', 'overlooking', 'the', 'lock', 'in', 'the', 'beer', 'garden', '.', 'can', \"'\", 't', 'believe', 'it', 'was']\n",
            "The joke is on the cops, niggers cant read.\n",
            "The query word nig is not a single token in sentence ['the', 'joke', 'is', 'on', 'the', 'cops', ',', 'niggers', 'cant', 'read', '.']\n",
            "10 Fall Weeknight Dinners Ready in Under an Hour (or Less!) \n",
            "The query word nig is not a single token in sentence ['10', 'fall', 'weeknight', 'dinners', 'ready', 'in', 'under', 'an', 'hour', '(', 'or', 'less', '!', ')']\n",
            "You're right.  Some of them just love to kill, so they will shoot a friendly dog at the drop of a hat.  This Spanish speaking nig-nog will probably not get into much trouble either, something like a $500 fine.\n",
            "The query word nig is not a single token in sentence ['you', \"'\", 're', 'right', '.', 'some', 'of', 'them', 'just', 'love', 'to', 'kill', ',', 'so', 'they', 'will', 'shoot', 'a', 'friendly', 'dog', 'at', 'the', 'drop']\n",
            "And to think this is non-fiction.  WTF.  Its been a bizarre and emotionally draining day for me, and I never left the house.  I've been on a roller coaster of emotions today.  Right now, I'm loopy.  I'm almost laughing at how pathetic we've become.  I can't do anything else.  I'll drive myself insane.  I'm so fucking sick of niggers, spics, pussy ass Asians, White traitors, the whole fucking bit.  I don't give a damn who the fuck calls me racist anymore.  I really don't give a shit.  \n",
            "The query word nig is not a single token in sentence ['and', 'to', 'think', 'this', 'is', 'non', '-', 'fiction', '.', 'wtf', '.', 'its', 'been', 'a', 'bizarre', 'and', 'emotionally', 'draining', 'day', 'for', 'me', ',']\n",
            "and the edomites did everything in retaliation,for what the jews did to them ,But I say fuck both Judahites. and edomites  sand niggers  \n",
            "The query word nig is not a single token in sentence ['and', 'the', 'edomites', 'did', 'everything', 'in', 'retaliation', ',', 'for', 'what', 'the', 'jews', 'did', 'to', 'them', ',', 'but', 'i', 'say', 'fuck', 'both']\n",
            "I'll wish it out loud.  I hope that snaggle tooth, nasty ass looking cunt is fucking dead.  And I hope it was extremely painful.  I may even pray tonight for her to languish in a more vegetative state than normal, then die after she shits herself a few times.  \n",
            "The query word nig is not a single token in sentence ['i', \"'\", 'll', 'wish', 'it', 'out', 'loud', '.', 'i', 'hope', 'that', 'snaggle', 'tooth', ',', 'nasty', 'ass', 'looking', 'cunt', 'is', 'fucking']\n",
            "Ye was real last night....on another note.\n",
            "The query word nig is not a single token in sentence ['ye', 'was', 'real', 'last', 'night', '.', '.', '.', '.', 'on', 'another', 'note', '.']\n",
            "Thanks but don't worry about it. I sometimes go to TOR to search but didn't tonight.\n",
            "The query word nig is not a single token in sentence ['thanks', 'but', 'don', \"'\", 't', 'worry', 'about', 'it', '.', 'i', 'sometimes', 'go', 'to', 'tor', 'to', 'search', 'but', 'didn', \"'\", 't', 'tonight', '.']\n"
          ]
        }
      ],
      "source": [
        "# Find all nearest neighbors for the banned sentences, up to 100 \n",
        "final_count = {}\n",
        "for banned_word, sentences in banned_sentences.items():\n",
        "  ctr_passed_sentences = 0\n",
        "  ctr_total_checked = 0\n",
        "  final_count[banned_word] = {}\n",
        "  for sentence in sentences:\n",
        "    if ctr_passed_sentences > 100 or ctr_total_checked > 1000:\n",
        "      break\n",
        "    ctr_total_checked = ctr_total_checked + 1\n",
        "    try:\n",
        "      # The model only tokenizes a certain character count, meaning there will be some false hits when trying to find even the word itself.\n",
        "      distances, neighbors, contexts = storage.query(query_sent=sentence, query_word=banned_word, k=5, filter_same_word=True)\n",
        "      # print('BANNED WORD: {} \\n ORIGINAL SENTENCE: {} '.format(banned_word, sentence))\n",
        "      for w in neighbors:\n",
        "          if w in final_count[banned_word]:\n",
        "            final_count[banned_word][w] = final_count[banned_word][w] + 1\n",
        "          else:\n",
        "            final_count[banned_word][w] = 1\n",
        "      # for d, w, c in zip(distances, neighbors, contexts):\n",
        "      ctr_passed_sentences = ctr_passed_sentences + 1\n",
        "          # print('{} {}  {}'.format(w, d, c.strip()))\n",
        "    except Exception as e:\n",
        "      continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXoBMEZtKn2P"
      },
      "outputs": [],
      "source": [
        "finalized_top_10 = {}\n",
        "\n",
        "for key, occur_dict in final_count.items():\n",
        "  if len(occur_dict.keys()) > 0:\n",
        "    # Sort occurances\n",
        "    sorted_dict = dict(sorted(occur_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "    # Take top 10\n",
        "    finalized_top_10[key] = {k: sorted_dict[k] for k in list(sorted_dict)[:10]}\n",
        "  else:\n",
        "    finalized_top_10[key] = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co-aLK-FMHFJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('top_10_100000.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(finalized_top_10, f, ensure_ascii=False, indent=4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4LBv95lL_WMy",
        "jmse5KnI_QxR"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "27ee02f7c83441dda9739228a11b2a15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d954ce4917e425588a1f5ec4e44eb59",
              "IPY_MODEL_0609395aa59a455eabd6169db701e237",
              "IPY_MODEL_8bd48da0c2674e6c91f842f0d479b56c"
            ],
            "layout": "IPY_MODEL_71cfe2496ad04fc2ab56556993fbb497"
          }
        },
        "7d954ce4917e425588a1f5ec4e44eb59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2020d8eb5f2421ab1380e0413f54cf1",
            "placeholder": "​",
            "style": "IPY_MODEL_79382c54e10244229829e9b96679f993",
            "value": "100%"
          }
        },
        "0609395aa59a455eabd6169db701e237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e745f55333245f68d6846ef73f265f0",
            "max": 4289,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe21129cee3b44c8a8318a2d3485da55",
            "value": 4289
          }
        },
        "8bd48da0c2674e6c91f842f0d479b56c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8f3afcd4d2e49f6864fda70350a1efc",
            "placeholder": "​",
            "style": "IPY_MODEL_04fa568012c545dfb8d8cc92f2bbbcd3",
            "value": " 4289/4289 [00:00&lt;00:00, 56081.48it/s]"
          }
        },
        "71cfe2496ad04fc2ab56556993fbb497": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2020d8eb5f2421ab1380e0413f54cf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79382c54e10244229829e9b96679f993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e745f55333245f68d6846ef73f265f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe21129cee3b44c8a8318a2d3485da55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8f3afcd4d2e49f6864fda70350a1efc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04fa568012c545dfb8d8cc92f2bbbcd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}